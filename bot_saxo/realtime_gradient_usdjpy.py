#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
fx_realtime_gradient_usdjpy.py - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ USDJPYå‹¾é…ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å–å¾—
å®Ÿè¡Œæ™‚ç‚¹ã§ã®ãƒ‰ãƒ«å††ã®å‹¾é…ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å³åº§ã«è¨ˆç®—ãƒ»è¡¨ç¤º
"""

import requests
import pandas as pd
import numpy as np
import os
import zipfile
import io
from datetime import datetime, timedelta
import logging
from config import TEST_TOKEN_24H, BASE_URL

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class RealtimeGradientUSDJPY:
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ USDJPYå‹¾é…ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å–å¾—ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.base_url = BASE_URL
        self.token = TEST_TOKEN_24H
        self.headers = {
            'Authorization': f'Bearer {self.token}',
            'Content-Type': 'application/json'
        }
        self.currency_pair = 'USDJPY'
        self.uic = None
        self.historical_data = None
        
        # åˆæœŸåŒ–
        self._initialize()
    
    def _initialize(self):
        """ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        logger.info("ğŸš€ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ USDJPYå‹¾é…ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å–å¾—ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–")
        
        # USDJPYã®UICå–å¾—
        self._get_usdjpy_uic()
        
        # éå»ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        self._load_historical_data()
    
    def _get_usdjpy_uic(self):
        """USDJPYã®UICå–å¾—"""
        try:
            params = {
                'Keywords': 'USDJPY',
                'AssetTypes': 'FxSpot',
                'limit': 1
            }
            response = requests.get(f"{self.base_url}/ref/v1/instruments", headers=self.headers, params=params)
            
            if response.status_code == 200:
                instruments = response.json()
                if instruments.get('Data'):
                    self.uic = instruments['Data'][0]['Identifier']
                    logger.info(f"âœ… USDJPY UICå–å¾—æˆåŠŸ: {self.uic}")
                else:
                    raise Exception("USDJPYãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            else:
                raise Exception(f"UICå–å¾—å¤±æ•—: {response.status_code}")
                
        except Exception as e:
            logger.error(f"UICå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def _load_historical_data(self):
        """éå»ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”¨ã¯è»½é‡ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰"""
        logger.info("ğŸ“Š ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‹¾é…è¨ˆç®—ç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™ä¸­...")
        
        try:
            # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‹¾é…è¨ˆç®—ã§ã¯éå»ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚’ã‚¹ã‚­ãƒƒãƒ—
            # ä»£ã‚ã‚Šã«ç¾åœ¨ä¾¡æ ¼ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ãŸçŸ­æœŸãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
            logger.info("âš¡ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”¨ï¼šè»½é‡ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆä¸­...")
            self._create_realtime_dummy_data()
            
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿æº–å‚™ã‚¨ãƒ©ãƒ¼: {e}")
            self._create_realtime_dummy_data()
    
    def _extract_recent_data(self, zip_path, days_back=3):
        """ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æœ€è¿‘ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        try:
            all_data = []
            cutoff_time = datetime.now() - timedelta(days=days_back)
            
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                csv_files = [f for f in zip_ref.namelist() if f.endswith('.csv')]
                
                for csv_file in csv_files:
                    with zip_ref.open(csv_file) as file:
                        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦è¡Œ
                        for encoding in ['utf-8', 'shift_jis', 'cp932']:
                            try:
                                content = file.read().decode(encoding)
                                df = pd.read_csv(io.StringIO(content))
                                
                                # ã‚«ãƒ©ãƒ åæ­£è¦åŒ–
                                column_mapping = {
                                    "æ—¥æ™‚": "timestamp",
                                    "å§‹å€¤(BID)": "open_bid",
                                    "é«˜å€¤(BID)": "high_bid", 
                                    "å®‰å€¤(BID)": "low_bid",
                                    "çµ‚å€¤(BID)": "close_bid",
                                    "å§‹å€¤(ASK)": "open_ask",
                                    "é«˜å€¤(ASK)": "high_ask",
                                    "å®‰å€¤(ASK)": "low_ask", 
                                    "çµ‚å€¤(ASK)": "close_ask"
                                }
                                df = df.rename(columns=column_mapping)
                                
                                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’å¤‰æ›
                                df['timestamp'] = pd.to_datetime(df['timestamp'])
                                
                                # æœ€è¿‘ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿
                                recent_df = df[df['timestamp'] >= cutoff_time]
                                
                                if not recent_df.empty:
                                    # ä¸­é–“ä¾¡æ ¼ã‚’è¨ˆç®—
                                    recent_df['close'] = (recent_df['close_bid'] + recent_df['close_ask']) / 2
                                    recent_df['high'] = (recent_df['high_bid'] + recent_df['high_ask']) / 2
                                    recent_df['low'] = (recent_df['low_bid'] + recent_df['low_ask']) / 2
                                    recent_df['open'] = (recent_df['open_bid'] + recent_df['open_ask']) / 2
                                    
                                    all_data.append(recent_df[['timestamp', 'open', 'high', 'low', 'close']])
                                break
                            except UnicodeDecodeError:
                                continue
            
            if all_data:
                combined_df = pd.concat(all_data, ignore_index=True)
                combined_df = combined_df.sort_values('timestamp').drop_duplicates().reset_index(drop=True)
                return combined_df
            else:
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return pd.DataFrame()
    
    def _create_realtime_dummy_data(self):
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”¨ãƒ‡ãƒ¼ã‚¿ä½œæˆï¼ˆååˆ†ãªãƒ‡ãƒ¼ã‚¿é‡ï¼‰"""
        logger.info("ğŸ”§ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆä¸­...")
        
        # å‹¾é…è¨ˆç®—ã«å¿…è¦ãªæœ€å°ãƒ‡ãƒ¼ã‚¿é‡ã‚’ç¢ºä¿
        # - MACDè¨ˆç®—: æœ€ä½26æœŸé–“å¿…è¦
        # - 1æ™‚é–“è¶³ãƒ‡ãƒ¼ã‚¿: æœ€ä½30æ™‚é–“åˆ†å¿…è¦
        # - å®‰å…¨ã®ãŸã‚48æ™‚é–“åˆ†ç”Ÿæˆ
        
        end_time = datetime.now().replace(second=0, microsecond=0)
        start_time = end_time - timedelta(hours=48)
        
        timestamps = pd.date_range(start=start_time, end=end_time, freq='1min')
        
        # ã‚ˆã‚Šç¾å®Ÿçš„ãªUSDJPYä¾¡æ ¼å¤‰å‹•ã‚’ç”Ÿæˆ
        np.random.seed(int(datetime.now().timestamp()) % 1000)
        base_price = 143.50
        
        # ã‚ˆã‚Šç¾å®Ÿçš„ãªä¾¡æ ¼å¤‰å‹•ï¼ˆæ—¥æœ¬æ™‚é–“ã®å¸‚å ´é–‹é–‰ã‚’è€ƒæ…®ï¼‰
        price_changes = []
        for i, ts in enumerate(timestamps):
            # å¸‚å ´æ™‚é–“ã‚’è€ƒæ…®ã—ãŸå¤‰å‹•å¹…èª¿æ•´
            hour = ts.hour
            if 7 <= hour <= 17:  # æ±äº¬ãƒ»ãƒ­ãƒ³ãƒ‰ãƒ³æ™‚é–“ï¼ˆãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£é«˜ï¼‰
                volatility = 0.008
            elif 22 <= hour or hour <= 6:  # NYæ™‚é–“ï¼ˆãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ä¸­ï¼‰
                volatility = 0.006
            else:  # ãã®ä»–ã®æ™‚é–“ï¼ˆãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ä½ï¼‰
                volatility = 0.003
            
            change = np.random.normal(0, volatility)
            price_changes.append(change)
        
        # ç´¯ç©å’Œã§ä¾¡æ ¼ç³»åˆ—ã‚’ç”Ÿæˆ
        prices = base_price + np.cumsum(price_changes)
        
        # å„ãƒ­ãƒ¼ã‚½ã‚¯è¶³ã®é«˜å€¤ãƒ»å®‰å€¤
        spreads = np.random.uniform(0.002, 0.012, len(timestamps))
        
        self.historical_data = pd.DataFrame({
            'timestamp': timestamps,
            'open': prices,
            'high': prices + spreads/2 + np.random.uniform(0, 0.005, len(timestamps)),
            'low': prices - spreads/2 - np.random.uniform(0, 0.005, len(timestamps)),
            'close': prices
        })
        
        logger.info(f"âœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”¨ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†: {len(self.historical_data)}è¡Œ (éå»{(end_time-start_time).total_seconds()/3600:.0f}æ™‚é–“åˆ†)")
        logger.info(f"   ä¾¡æ ¼ç¯„å›²: {self.historical_data['close'].min():.3f} ï½ {self.historical_data['close'].max():.3f}")
        
        # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
        self._check_data_quality()
    
    def _check_data_quality(self):
        """ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯"""
        try:
            # æ™‚é–“è»¸åˆ¥ãƒ‡ãƒ¼ã‚¿æ•°ã‚’ãƒã‚§ãƒƒã‚¯
            df = self.historical_data.set_index('timestamp')
            
            check_results = {}
            timeframes = {
                '1min': '1min',
                '5min': '5min', 
                '15min': '15min',
                '1hour': '1h'
            }
            
            for tf_name, freq in timeframes.items():
                resampled = df.resample(freq).agg({
                    'close': 'last'
                }).dropna()
                check_results[tf_name] = len(resampled)
            
            logger.info("ğŸ“Š æ™‚é–“è»¸åˆ¥ãƒ‡ãƒ¼ã‚¿æ•°ãƒã‚§ãƒƒã‚¯:")
            for tf, count in check_results.items():
                status = "âœ…" if count >= 30 else "âš ï¸" if count >= 10 else "âŒ"
                logger.info(f"   {tf:>6}: {count:>3}æœ¬ {status}")
            
            # MACDè¨ˆç®—ã«å¿…è¦ãªæœ€å°ãƒ‡ãƒ¼ã‚¿æ•°ãƒã‚§ãƒƒã‚¯
            min_required = 30  # MACD + ä½™è£•
            all_sufficient = all(count >= min_required for count in check_results.values())
            
            if all_sufficient:
                logger.info("âœ… å…¨æ™‚é–“è»¸ã§ååˆ†ãªãƒ‡ãƒ¼ã‚¿é‡ã‚’ç¢ºä¿")
            else:
                logger.warning("âš ï¸  ä¸€éƒ¨æ™‚é–“è»¸ã§ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®å¯èƒ½æ€§")
            
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _create_dummy_data(self):
        """å¾“æ¥ã®ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ä½œæˆï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚æ®‹ã™ï¼‰"""
        self._create_realtime_dummy_data()
    
    def get_current_price(self):
        """ç¾åœ¨ã®USDJPYä¾¡æ ¼ã‚’å–å¾—"""
        try:
            params = {
                'Uic': str(self.uic),
                'AssetType': 'FxSpot',
                'FieldGroups': 'Quote'
            }
            response = requests.get(f"{self.base_url}/trade/v1/infoprices", headers=self.headers, params=params)
            
            if response.status_code == 200:
                prices = response.json()
                
                if prices.get('Data'):
                    quote = prices['Data'][0].get('Quote', {})
                elif 'Quote' in prices:
                    quote = prices['Quote']
                else:
                    return None
                
                bid = quote.get('Bid', 0)
                ask = quote.get('Ask', 0)
                current_price = (bid + ask) / 2
                
                logger.info(f"ğŸ’¹ ç¾åœ¨ä¾¡æ ¼: BID={bid}, ASK={ask}, ä¸­é–“ä¾¡æ ¼={current_price:.3f}")
                return current_price
            else:
                logger.error(f"ä¾¡æ ¼å–å¾—å¤±æ•—: {response.status_code}")
                return None
                
        except Exception as e:
            logger.error(f"ä¾¡æ ¼å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def add_current_price_to_history(self, current_price):
        """ç¾åœ¨ä¾¡æ ¼ã‚’å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ """
        if current_price is None:
            return
        
        try:
            # ç¾åœ¨æ™‚åˆ»ã®1åˆ†è¶³ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
            current_time = datetime.now().replace(second=0, microsecond=0)
            
            new_row = {
                'timestamp': current_time,
                'open': current_price,
                'high': current_price,
                'low': current_price,
                'close': current_price
            }
            
            # å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ 
            self.historical_data = pd.concat([
                self.historical_data,
                pd.DataFrame([new_row])
            ], ignore_index=True)
            
            # é‡è¤‡é™¤å»ã¨ã‚½ãƒ¼ãƒˆ
            self.historical_data = self.historical_data.drop_duplicates(subset=['timestamp']).sort_values('timestamp').reset_index(drop=True)
            
            logger.info(f"ğŸ“Š å±¥æ­´ãƒ‡ãƒ¼ã‚¿æ›´æ–°: {current_time} = {current_price:.3f}")
            
        except Exception as e:
            logger.error(f"å±¥æ­´ãƒ‡ãƒ¼ã‚¿æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
    
    def resample_to_timeframes(self):
        """å„æ™‚é–“è»¸ã«ãƒªã‚µãƒ³ãƒ—ãƒ«ï¼ˆè­¦å‘Šå¯¾å¿œç‰ˆï¼‰"""
        try:
            df = self.historical_data.set_index('timestamp')
            
            timeframes = {
                '1min': df.resample('1min').agg({
                    'open': 'first',
                    'high': 'max',
                    'low': 'min',
                    'close': 'last'
                }).dropna(),
                
                '5min': df.resample('5min').agg({
                    'open': 'first',
                    'high': 'max',
                    'low': 'min',
                    'close': 'last'
                }).dropna(),
                
                '15min': df.resample('15min').agg({
                    'open': 'first',
                    'high': 'max',
                    'low': 'min',
                    'close': 'last'
                }).dropna(),
                
                '1hour': df.resample('1h').agg({
                    'open': 'first',
                    'high': 'max',
                    'low': 'min',
                    'close': 'last'
                }).dropna()
            }
            
            return timeframes
            
        except Exception as e:
            logger.error(f"ãƒªã‚µãƒ³ãƒ—ãƒ«ã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def calculate_macd_gradient(self, df, fast=12, slow=26):
        """MACDå‹¾é…è¨ˆç®—"""
        try:
            if len(df) < slow + 5:
                return 0.0
            
            # EMAè¨ˆç®—
            ema_fast = df['close'].ewm(span=fast, adjust=False).mean()
            ema_slow = df['close'].ewm(span=slow, adjust=False).mean()
            
            # MACDãƒ©ã‚¤ãƒ³
            macd_line = ema_fast - ema_slow
            
            # æœ€æ–°ã®å‹¾é…ï¼ˆ5æœŸé–“ã®å¤‰åŒ–ç‡ï¼‰
            if len(macd_line) >= 5:
                current_macd = macd_line.iloc[-1]
                past_macd = macd_line.iloc[-5]
                
                if past_macd != 0:
                    gradient = ((current_macd - past_macd) / abs(past_macd)) * 100
                    return max(-100, min(100, gradient))
            
            return 0.0
            
        except Exception as e:
            logger.error(f"MACDå‹¾é…è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return 0.0
    
    def calculate_ma_gradient(self, df, period=20):
        """ç§»å‹•å¹³å‡å‹¾é…è¨ˆç®—"""
        try:
            if len(df) < period + 5:
                return 0.0
            
            # ç§»å‹•å¹³å‡
            ma = df['close'].rolling(window=period).mean()
            
            # æœ€æ–°ã®å‹¾é…ï¼ˆ5æœŸé–“ã®å¤‰åŒ–ç‡ï¼‰
            if len(ma) >= 5:
                current_ma = ma.iloc[-1]
                past_ma = ma.iloc[-5]
                
                if past_ma != 0:
                    gradient = ((current_ma - past_ma) / past_ma) * 100
                    return max(-100, min(100, gradient))
            
            return 0.0
            
        except Exception as e:
            logger.error(f"MAå‹¾é…è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return 0.0
    
    def calculate_atr_gradient(self, df, period=14):
        """ATRå‹¾é…è¨ˆç®—"""
        try:
            if len(df) < period + 5:
                return 0.0
            
            # True Rangeè¨ˆç®—
            df_copy = df.copy()
            df_copy['prev_close'] = df_copy['close'].shift(1)
            df_copy['tr1'] = df_copy['high'] - df_copy['low']
            df_copy['tr2'] = abs(df_copy['high'] - df_copy['prev_close'])
            df_copy['tr3'] = abs(df_copy['low'] - df_copy['prev_close'])
            
            df_copy['true_range'] = df_copy[['tr1', 'tr2', 'tr3']].max(axis=1)
            
            # ATRè¨ˆç®—
            atr = df_copy['true_range'].rolling(window=period).mean()
            
            # æœ€æ–°ã®å‹¾é…ï¼ˆ5æœŸé–“ã®å¤‰åŒ–ç‡ï¼‰
            if len(atr) >= 5:
                current_atr = atr.iloc[-1]
                past_atr = atr.iloc[-5]
                
                if past_atr != 0:
                    gradient = ((current_atr - past_atr) / past_atr) * 100
                    return max(-100, min(100, gradient))
            
            return 0.0
            
        except Exception as e:
            logger.error(f"ATRå‹¾é…è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return 0.0
    
    def calculate_price_gradient(self, df):
        """ä¾¡æ ¼å‹¾é…è¨ˆç®—"""
        try:
            if len(df) < 5:
                return 0.0
            
            # ä¾¡æ ¼ã®å¤‰åŒ–ç‡
            current_price = df['close'].iloc[-1]
            past_price = df['close'].iloc[-5]
            
            if past_price != 0:
                gradient = ((current_price - past_price) / past_price) * 100
                return max(-100, min(100, gradient))
            
            return 0.0
            
        except Exception as e:
            logger.error(f"ä¾¡æ ¼å‹¾é…è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return 0.0
    
    def calculate_realtime_gradients(self):
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‹¾é…ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨ˆç®—ï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        logger.info("ğŸ“ˆ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‹¾é…ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨ˆç®—é–‹å§‹")
        
        # ç¾åœ¨ä¾¡æ ¼å–å¾—
        current_price = self.get_current_price()
        
        # å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã«ç¾åœ¨ä¾¡æ ¼ã‚’è¿½åŠ 
        self.add_current_price_to_history(current_price)
        
        # æ™‚é–“è»¸åˆ¥ãƒªã‚µãƒ³ãƒ—ãƒ«
        timeframes = self.resample_to_timeframes()
        
        if not timeframes:
            logger.error("âŒ æ™‚é–“è»¸ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå¤±æ•—")
            return None
        
        # ãƒ‡ãƒ¼ã‚¿æ•°ç¢ºèª
        logger.info("ğŸ“Š ãƒªã‚µãƒ³ãƒ—ãƒ«å¾Œãƒ‡ãƒ¼ã‚¿æ•°:")
        for tf_name, df in timeframes.items():
            logger.info(f"   {tf_name:>6}: {len(df):>3}æœ¬")
        
        # å„æ™‚é–“è»¸ã®å‹¾é…è¨ˆç®—
        gradients = {}
        
        for tf_name, df in timeframes.items():
            if df.empty or len(df) < 10:  # æœ€å°ãƒ‡ãƒ¼ã‚¿æ•°ãƒã‚§ãƒƒã‚¯
                logger.warning(f"   {tf_name}: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ï¼ˆ{len(df)}æœ¬ï¼‰â†’ 0.0ã‚’ä½¿ç”¨")
                gradients[tf_name] = 0.0
                continue
            
            # å„æŒ‡æ¨™ã®å‹¾é…ã‚’è¨ˆç®—
            macd_grad = self.calculate_macd_gradient(df)
            ma_grad = self.calculate_ma_gradient(df)
            atr_grad = self.calculate_atr_gradient(df)
            price_grad = self.calculate_price_gradient(df)
            
            # è¤‡åˆå‹¾é…ï¼ˆå„æŒ‡æ¨™ã®é‡ã¿ä»˜ã‘å¹³å‡ï¼‰
            composite_gradient = (macd_grad * 0.3 + ma_grad * 0.3 + atr_grad * 0.2 + price_grad * 0.2)
            gradients[tf_name] = round(composite_gradient, 2)
            
            logger.info(f"   {tf_name}: MACD={macd_grad:>6.2f}, MA={ma_grad:>6.2f}, ATR={atr_grad:>6.2f}, Price={price_grad:>6.2f} â†’ è¤‡åˆ={composite_gradient:>6.2f}")
        
        # 4æ™‚é–“è»¸ãƒ‘ã‚¿ãƒ¼ãƒ³
        pattern = [
            gradients.get('1min', 0.0),
            gradients.get('5min', 0.0),
            gradients.get('15min', 0.0),
            gradients.get('1hour', 0.0)
        ]
        
        return {
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'current_price': current_price,
            'gradient_pattern': pattern,
            'detailed_gradients': gradients,
            'data_quality': {tf: len(df) for tf, df in timeframes.items()}
        }
    
    def display_results(self, result):
        """çµæœã‚’è¦‹ã‚„ã™ãè¡¨ç¤ºï¼ˆãƒ‡ãƒ¼ã‚¿å“è³ªæƒ…å ±ä»˜ãï¼‰"""
        if not result:
            print("âŒ å‹¾é…è¨ˆç®—çµæœãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        print("=" * 70)
        print("ğŸ”¥ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ USDJPYå‹¾é…ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
        print("=" * 70)
        print(f"â° å–å¾—æ™‚åˆ»: {result['timestamp']}")
        print(f"ğŸ’¹ ç¾åœ¨ä¾¡æ ¼: {result['current_price']:.3f}")
        print()
        
        # ãƒ‡ãƒ¼ã‚¿å“è³ªæƒ…å ±
        if 'data_quality' in result:
            print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿å“è³ª:")
            for tf, count in result['data_quality'].items():
                status = "âœ…" if count >= 30 else "âš ï¸" if count >= 10 else "âŒ"
                print(f"   {tf:>6}: {count:>3}æœ¬ {status}")
            print()
        
        # numpyå‹ã‚’é€šå¸¸ã®floatã«å¤‰æ›
        pattern_clean = [float(x) for x in result['gradient_pattern']]
        print("ğŸ“Š å‹¾é…ãƒ‘ã‚¿ãƒ¼ãƒ³ [1åˆ†è¶³, 5åˆ†è¶³, 15åˆ†è¶³, 1æ™‚é–“è¶³]:")
        print(f"   {pattern_clean}")
        print()
        print("ğŸ“ˆ è©³ç´°å‹¾é…:")
        for tf, grad in result['detailed_gradients'].items():
            grad_float = float(grad)
            trend = "ğŸ”´" if grad_float < -10 else "ğŸŸ¡" if grad_float < 10 else "ğŸŸ¢"
            print(f"   {tf:>6}: {grad_float:>8.2f}% {trend}")
        print()
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ¤å®š
        avg_gradient = sum(pattern_clean) / 4
        if avg_gradient > 15:
            trend_status = "ğŸš€ å¼·ã„ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰"
        elif avg_gradient > 5:
            trend_status = "ğŸ“ˆ ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰"
        elif avg_gradient > -5:
            trend_status = "â¡ï¸  ãƒ¬ãƒ³ã‚¸ç›¸å ´"
        elif avg_gradient > -15:
            trend_status = "ğŸ“‰ ä¸‹é™ãƒˆãƒ¬ãƒ³ãƒ‰"
        else:
            trend_status = "ğŸ’¥ å¼·ã„ä¸‹é™ãƒˆãƒ¬ãƒ³ãƒ‰"
        
        print(f"ğŸ¯ ç·åˆåˆ¤å®š: {trend_status} (å¹³å‡å‹¾é…: {avg_gradient:.2f}%)")
        print("=" * 70)
        
        # å‹¾é…ã®è§£é‡ˆèª¬æ˜
        print("\nğŸ“ å‹¾é…ã®è§£é‡ˆ:")
        print("   ğŸŸ¢ +10%ä»¥ä¸Š: å¼·ã„ä¸Šæ˜‡åœ§åŠ›")
        print("   ğŸŸ¡ -10%ï½+10%: å®‰å®šãƒ»ãƒ¬ãƒ³ã‚¸")  
        print("   ğŸ”´ -10%ä»¥ä¸‹: å¼·ã„ä¸‹é™åœ§åŠ›")
        print(f"\nğŸ” ç¾åœ¨ã®ä¾¡æ ¼: {result['current_price']:.3f} (ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å–å¾—)")
        
        # ãƒ‡ãƒ¼ã‚¿å“è³ªã«åŸºã¥ãä¿¡é ¼æ€§è©•ä¾¡
        if 'data_quality' in result:
            min_data = min(result['data_quality'].values())
            if min_data >= 30:
                reliability = "é«˜"
                emoji = "âœ…"
            elif min_data >= 15:
                reliability = "ä¸­"
                emoji = "âš ï¸"
            else:
                reliability = "ä½"
                emoji = "âŒ"
            
            print(f"\nğŸ“Š å‹¾é…ä¿¡é ¼æ€§: {emoji} {reliability} (æœ€å°ãƒ‡ãƒ¼ã‚¿æ•°: {min_data}æœ¬)")
        
        print("=" * 70)

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    try:
        # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        gradient_system = RealtimeGradientUSDJPY()
        
        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‹¾é…è¨ˆç®—
        result = gradient_system.calculate_realtime_gradients()
        
        # çµæœè¡¨ç¤º
        gradient_system.display_results(result)
        
    except Exception as e:
        logger.error(f"å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()